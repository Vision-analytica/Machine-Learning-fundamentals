{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression in Machine Learning\n",
    "\n",
    "Linear regression is a popular and widely used algorithm in machine learning for predicting continuous numeric values. It models the relationship between independent variables (input features) and a dependent variable (target variable) by fitting a linear equation to the observed data. In this section, we will provide a brief overview of linear regression, including the mathematical explanation and figures to aid understanding.\n",
    "\n",
    "<img src=\"ML-image/Linear-reg1.png\" width=\"350\" height=\"300\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation:\n",
    "The linear regression algorithm aims to find the best-fit line that represents the relationship between the input features ($x$) and the target variable ($y$). The equation for a simple linear regression can be expressed as: \n",
    "\n",
    "$y=mx+b$\n",
    "\n",
    "where\n",
    "\n",
    "- $y$ represents the target variable or the dependent variable we want to predict.\n",
    "- $x$ represents the input feature or the independent variable.\n",
    "- $m$ represents the slope of the line, which represents the rate of change of $y$ with respect to $x$.\n",
    "- $b$ represents the $y$-intercept, which is the value of $y$ when $x$ is equal to $0$.\n",
    "\n",
    "<img src=\"ML-image/Linear-reg0.png\" width=\"350\" height=\"300\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship of regression lines\n",
    "\n",
    "- A linear line showing the relationship between the dependent and independent variables is called a regression line. \n",
    "- A regression line can show two types of relationship:\n",
    "\n",
    "1. **Positive Linear Relationship:** If the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship.\n",
    "\n",
    "<img src=\"ML-image/pos-lin-reg.png\" width=\"300\" height=\"250\" />\n",
    "\n",
    "1. **Negative Linear Relationship:** If the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.\n",
    "\n",
    "<img src=\"ML-image/neg-lin-reg.png\" width=\"300\" height=\"250\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Linear Regression\n",
    "\n",
    "Linear regression can be further divided into two types of the algorithm:\n",
    "\n",
    "1. **Simple Linear Regression:** If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n",
    "2. **Multiple Linear regression:** If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple linear regression \n",
    "\n",
    "Simple linear regression (SLR) is a statistical model that focuses on the relationship between a single independent variable (or feature) and an outcome variable. In SLR, the functional relationship between the outcome variable and the regression coefficient is linear. This means that the mathematical function describing the relationship is a straight line and the impact of the independent variable on the outcome is represented by a linear regression coefficient.\n",
    "\n",
    "**Example:** predicting housing prices based on features such as the number of bedrooms, square footage, and location.\n",
    "\n",
    "#### 1.1. Mathematical Explanation: \n",
    "Mathematically, we can represent a linear regression as:\n",
    "\n",
    "$y=\\beta_0 + \\beta_1 x +\\epsilon$\n",
    "\n",
    "where\n",
    "\n",
    "- $y$ = Dependent Variable (Target Variable)\n",
    "- $x$ = Independent Variable (predictor Variable)\n",
    "- $\\beta_0$ = intercept of the line (Gives an additional degree of freedom)\n",
    "- $\\beta_1$ = Linear regression coefficient (scale factor to each input value).\n",
    "- $\\epsilon$ = random error\n",
    "\n",
    "The goal of linear regression is to estimate the values of the regression coefficients\n",
    "\n",
    "<img src=\"ML-image/Multi-lin-reg.png\" width=\"500\" height=\"350\" />\n",
    "\n",
    "This algorithm explains the linear relationship between the dependent(output) variable $y$ and the independent(predictor) variable $x$ using a straight line  $y=\\beta_0+\\beta_1 x$.\n",
    "\n",
    "#### 1.2. Goal\n",
    "\n",
    "- The goal of the linear regression algorithm is to get the best values for $\\beta_0$ and $\\beta_1$ to find the best fit line. \n",
    "- The best fit line is a line that has the least error which means the error between predicted values and actual values should be minimum.\n",
    "- For a datset with $n$ observation $(x_i, y_i)$, where $i=1,2,3...., n$ the above function can be written as follows\n",
    "\n",
    "    $y_i=\\beta_0 + \\beta_1 x_i +\\epsilon_i$\n",
    "\n",
    "    where $y_i$ is the value of the observation of the dependent variable (outcome variable) in the smaple, $x_i$ is the value of $ith$ observation of the independent variable or feature in the sample, $\\epsilon_i$ is the random error (also known as residuals) in predicting the value of $y_i$, $\\beta_0$ and $\\beta_i$ are the regression parameters (or regression coefficients or feature weights).\n",
    "\n",
    "#### 1.3. Calculating the regression parameters\n",
    "\n",
    "In simple linear regression, there is only one independent variable ($x$) and one dependent variable ($y$). The parameters (coefficients) in simple linear regression can be calculated using the method of **ordinary least squares (OLS)**. The equations and formulas involved in calculating the parameters are as follows:\n",
    "\n",
    "1. **Model Representation:**\n",
    "\n",
    "    The simple linear regression model can be represented as:\n",
    "    $y = \\beta_0 + \\beta_1 x + \\epsilon$\n",
    "\n",
    "    So,\n",
    "\n",
    "    $\\epsilon = y -\\beta_0 - \\beta_1 x$.\n",
    "\n",
    "2. **Cost Function or mean squared error (MSE):**\n",
    "\n",
    "    The MSE, measures the average squared difference between the predicted values ($\\hat{y}$) and the actual values of the dependent variable ($y$). It is given by:\n",
    "\n",
    "    MSE = $\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $n$ is the number of data points.\n",
    "    - $y_i$ is the actual value of the dependent variable for the i-th data point.\n",
    "    - $\\hat{y}_i$ is the predicted value of the dependent variable for the $i-th$ data point.\n",
    "\n",
    "3. **Minimization of the Cost Function:**\n",
    "\n",
    "    The parameters $\\beta_0$ and $\\beta_1$ are estimated by minimizing the cost function. The formulas for calculating the parameter estimates are derived from the derivative of the cost function with respect to each parameter.\n",
    "\n",
    "    The parameter estimates are given by:\n",
    "\n",
    "    - $\\hat{\\beta_1} = \\frac{\\text{Cov}(x,y)}{Var(x)}$ \n",
    "    - $\\hat{\\beta_0} = \\text{y} - \\hat{\\beta_1}\\times \\text{mean}(x)$ \n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{\\beta_0}$ is the estimated $y$-intercept.\n",
    "    - $\\hat{\\beta_1}$ is the estimated slope.\n",
    "    - Cov$(x, y)$ is the covariance between $x$ and $y$.\n",
    "    - $\\text{Var}(x)$ is the variance of $x$.\n",
    "    - $\\text{mean}(x)$ is the mean of $x$.\n",
    "    - $\\text{mean}(y)$ is the mean of $y$.\n",
    "\n",
    "    The estimated parameters $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ provide the values of the intercept and slope that best fit the data according to the simple linear regression model.\n",
    "\n",
    "4. **Prediction:**\n",
    "\n",
    "    Once the parameter estimates are obtained, predictions can be made using the equation:\n",
    "\n",
    "    $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{y}$ is the predicted value of the dependent variable.\n",
    "    - $\\hat{\\beta_0}$ is the estimated y-intercept.\n",
    "    - $\\hat{\\beta_1}$ is the estimated slope.\n",
    "    - $x$ is the value of the independent variable for which the prediction is being made.\n",
    "\n",
    "    These equations and formulas allow for the calculation of the parameters in simple linear regression using the method of **ordinary least squares (OLS)**. By minimizing the sum of squared differences between predicted and actual values, the parameters are determined to best fit the data and enable prediction of the dependent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiple linear regression algorithms\n",
    "\n",
    "Multiple Linear Regression is an extension of Simple Linear Regression that allows for the analysis of the relationship between a dependent variable ($y$) and multiple independent variables ($x_1$, $x_2$, ..., $x_p$). It assumes a linear relationship between the variables, with the objective of finding the best-fit hyperplane in a multi-dimensional space.\n",
    "\n",
    "#### 2.1. Mathematical Explanation:\n",
    "\n",
    "The multiple linear regression model can be expressed mathematically as:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$ is the dependent variable.\n",
    "- $x_1$, $x_2$, ...., $x_p$ are the independent variables.\n",
    "- $\\beta_0$ is the $y$-intercept, representing the value of $y$ when all the independent variables are zero.\n",
    "- $\\beta_1$, $\\beta_2$, ...,  $\\beta_p$ are the coefficients associated with each independent variable, indicating the impact of each variable on the dependent variable while holding other variables constant.\n",
    "- $\\epsilon$ is the error term, representing the random variability or noise in the relationship between the variables.\n",
    "\n",
    "#### 2.2. Goal \n",
    "The goal of multiple linear regression is to estimate the values of $\\beta_0$, $\\beta_1$, $\\beta_2$, ...,  $\\beta_p$ that provide the best fit to the observed data, minimizing the sum of squared differences between the predicted values ($\\hat{y}$) and the actual values of $y$.\n",
    "\n",
    "#### 2.3. Calculating the regression parameters \n",
    "\n",
    "In linear regression, the parameters (coefficients) are estimated using a method called ordinary least squares (OLS). The goal of OLS is to find the values of the parameters that minimize the sum of squared differences between the predicted values and the actual values of the dependent variable. The equations and formulas involved in calculating the linear regression parameters are as follows:\n",
    "\n",
    "1. **Cost Function or mean squared error (MSE):**\n",
    "\n",
    "    The cost function, measures the average squared difference between the predicted values ($\\hat{y}$) and the actual values of the dependent variable ($y$). It is given by:\n",
    "\n",
    "    $\\text{MSE} = \\frac{1}{n} \\sum(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $n$ is the number of data points.\n",
    "    - $y_i$ is the actual value of the dependent variable for the $i-th$ data point.\n",
    "    - $\\hat{y}_i$ is the predicted value of the dependent variable for the $i-th$ data point.\n",
    "\n",
    "2. **Minimization of the Cost Function:**\n",
    "    The parameters $\\beta_0$, $\\beta_1$, $\\beta_2$, ...,  $\\beta_p$ are estimated by minimizing the cost function. The formulas for calculating the parameter estimates are derived from the derivative of the cost function with respect to each parameter.\n",
    "\n",
    "    The parameter estimates are given by:\n",
    "\n",
    "    $\\hat{\\beta} = \\left(X^T X\\right)^{-1}\\, X^T y$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{\\beta}$ is the vector of parameter estimates.\n",
    "    - $X$ is the design matrix consisting of the independent variables.\n",
    "    - $X^T$ is the transpose of the design matrix.\n",
    "    - $\\left(X^T X \\right)^{-1}$ is the inverse of the matrix product $X^TX$.\n",
    "    - $y$ is the vector of the dependent variable values.\n",
    "    \n",
    "    The estimated parameter vector $\\hat{\\beta}$ provides the values of the coefficients that best fit the data according to the linear regression model.\n",
    "\n",
    "3. **Prediction:**\n",
    "    Once the parameter estimates are obtained, predictions can be made using the equation:\n",
    "\n",
    "    $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + ... + \\hat{\\beta_p} x_p$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{y}$ is the predicted value of the dependent variable.\n",
    "    - $\\hat{\\beta_0}$, $\\hat{\\beta_1}$, $\\hat{\\beta_2}$, ...,  $\\hat{\\beta_p}$ are the estimated coefficients.\n",
    "    - $x_1$, $x_2$, ..., $x_p$ are the values of the independent variables for which the prediction is being made.\n",
    "    \n",
    "    These equations and formulas form the basis of ordinary least squares (OLS) estimation for linear regression. By minimizing the sum of squared differences between predicted and actual values, the parameters are determined to best fit the data and allow for prediction of the dependent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE on Gradient Descent for Linear Regression:**\n",
    "> A regression model optimizes the gradient descent algorithm to update the coefficients of the line by reducing the cost function by randomly selecting coefficient values and then iteratively updating the values to reach the minimum cost function.\n",
    "> \n",
    "> Gradient Descent is an iterative optimization algorithm commonly used in machine learning to find the optimal parameters in a model. It can also be applied to linear regression to estimate the parameters (coefficients) that minimize the cost function.\n",
    ">\n",
    "> The steps involved in using Gradient Descent for Linear Regression are as follows:\n",
    ">\n",
    "> **Define the Cost Function:**\n",
    "> The cost function for linear regression is the Mean Squared Error (MSE), which measures the average squared difference between the predicted values (ŷ) and the actual values (y) of the dependent variable.\n",
    ">\n",
    "> $MSE = \\frac{1}{2n} \\sum (y_i - \\hat{y}_i)^2$\n",
    "> \n",
    "> Where:\n",
    "> \n",
    "> - $n$ is the number of data points.\n",
    "> - $y_i$ is the actual value of the dependent variable for the i-th data point.\n",
    "> $\\hat{y}_i$ is the predicted value of the dependent variable for the i-th data point.\n",
    "> \n",
    "> **Initialize the Parameters:**\n",
    "> \n",
    "> Start by initializing the parameters (coefficients) with random values. Typically, they are initialized as zero or small random values.\n",
    ">\n",
    "> **Calculate the Gradient:**\n",
    "> Compute the gradient of the cost function with respect to each parameter. The gradient represents the direction of steepest ascent in the cost function space.\n",
    ">\n",
    "> $\\frac{\\partial (MSE)}{\\partial \\beta_0} = \\frac{1}{n}\\sum (\\hat{y}_i - y_i)$\n",
    ">\n",
    "> $\\frac{\\partial (MSE)}{\\partial \\beta_1} = \\frac{1}{n}\\sum (\\hat{y}_i - y_i)\\times x_i$\n",
    ">\n",
    "> Where:\n",
    ">\n",
    "> - $\\frac{\\partial (MSE)}{\\partial \\beta_0}$ is the gradient with respect to the y-intercept parameter ($\\beta_0$).\n",
    "> - $\\frac{\\partial (MSE)}{\\partial \\beta_1}$ is the gradient with respect to the slope parameter ($\\beta_1$).\n",
    "> - $\\hat{y}_i$ is the predicted value of the dependent variable for the i-th data point.\n",
    "> - $y_i$ is the actual value of the dependent variable for the i-th data point.\n",
    "> - $x_i$ is the value of the independent variable for the i-th data point.\n",
    ">\n",
    "> **Update the Parameters:**\n",
    "> Update the parameters using the gradient and a learning rate ($\\alpha$), which determines the step size in each iteration.\n",
    ">\n",
    "> - $\\beta_0 = \\beta_0 - \\alpha \\times \\frac{\\partial (MSE)}{\\partial \\beta_0}$\n",
    "> - $\\beta_1 = \\beta_1 - \\alpha \\times \\frac{\\partial (MSE)}{\\partial \\beta_1}$\n",
    "> \n",
    "> Repeat this update process for a specified number of iterations or until the change in the cost function becomes sufficiently small.\n",
    "> \n",
    "> **Predict:**\n",
    "> Once the parameters have converged or reached the desired number of iterations, use the final parameter values to make predictions on new data.\n",
    ">\n",
    "> $\\hat{y} = \\beta_0 +\\beta_1 x$\n",
    ">\n",
    "> Where:\n",
    ">\n",
    "> - $\\hat{y}$ is the predicted value of the dependent variable.\n",
    "> - $\\beta_0$ is the $y$-intercept parameter.\n",
    "> - $\\beta_1$ is the slope parameter.\n",
    "> - $x$ is the value of the independent variable for which the prediction is being made.\n",
    ">\n",
    "> Gradient Descent iteratively adjusts the parameters by updating them in the direction of the negative gradient until it reaches a minimum point in the cost function. This process allows for the estimation of optimal parameters in linear regression, enabling the model to make accurate predictions on unseen data.\n",
    ">\n",
    ">   <img src=\"ML-image/optimal-reg2.png\" width=\"1000\" height=\"320\" />\n",
    ">\n",
    "> Let’s take an example to understand this. If we want to go from top left point of the shape to bottom of the pit, a discrete number of steps can be taken to reach the bottom. \n",
    "> - If you decide to take larger steps each time, you may achieve the bottom sooner but, there’s a probability that you could overshoot the bottom of the pit and not even near the bottom. \n",
    "> - In the gradient descent algorithm, the number of steps you’re taking can be considered as the learning rate, and this decides how fast the algorithm converges to the minima.\n",
    ">\n",
    "> **In the gradient descent algorithm, the number of steps you’re taking can be considered as the learning rate i.e. $\\alpha$, and this decides how fast the algorithm converges to the minima.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in building a regression model\n",
    "\n",
    "- **STEP 1: Collect/Extract Data:** The first step in building a regression model is to collect or extract data on the dependent (outcome) variable and independent (feature) variables from different data sources.\n",
    "\n",
    "- **STEP 2: Pre-Process the Data:** Before the model is built, it is essential to ensure the quality of the data for issues such as reliability, com- pleteness, usefulness, accuracy, missing data, and outliers.\n",
    "\n",
    "    - Data imputation techniques may be used to deal with missing data. Use of descriptive statistics and visualization (such as box plot and scatter plot) may be used to identify the existence of outliers and variability in the dataset.\n",
    "    - Many new variables (such as the ratio of variables or product of variables) can be derived (aka feature engineering) and also used in model building.\n",
    "    - Categorical data must be pre-processed using dummy variables as a part of feature engineering, prior to utilizing it in a regression model.\n",
    "\n",
    "- **STEP 3: Dividing Data into Training and Validation Datasets:** In this stage the data is divided into two subsets (sometimes more than two subsets):\n",
    "\n",
    "    - training dataset and\n",
    "    - validation or test dataset.\n",
    "    \n",
    "    The proportion of training dataset is usually between 70% and 80% of the data and the remaining data is treated as the validation data. The subsets may be created using random/ stratified sampling procedure. This is an important step to measure the performance of the model using dataset not used in model building. It is also essential to check for any overfitting of the model. In many cases, multiple training and multiple test data are used (called cross-validation).\n",
    "\n",
    "- **STEP 4: Perform Descriptive Analytics or Data Exploration:** It is always a good practice to perform descriptive analytics before moving to building a predictive analytics model. Descriptive statistics will help us to understand the variability in the model and visualization of the data through, say, a box plot which will show if there are any outliers in the data. Another visualization technique, the scatter plot, may also reveal if there is any obvious relationship between the two variables under consideration. Scatter plot is useful to describe the functional relationship between the dependent or outcome variable and features.\n",
    "\n",
    "- **STEP 5: Build the Model:** The model is built using the training dataset to estimate the regression parameters. The method of Ordinary Least Squares (OLS) is used to estimate the regression parameters.\n",
    "\n",
    "- **STEP 6: Perform Model Diagnostics:** Regression is often misused since many times the modeler fails to perform necessary diagnostics tests before applying the model. Before it can be applied, it is necessary that the model created is validated for all model assumptions including the definition of the function form. If the model assumptions are violated, then the modeler must use remedial measure.\n",
    "\n",
    "- **STEP 7: Validate the Model and Measure Model Accuracy:** A major concern in analytics is over-fitting, that is, the model may perform very well on the training dataset, but may perform badly in validation dataset. It is important to ensure that the model perfor- mance is consistent on the validation dataset as is in the training dataset. In fact, the model may be cross- validated using multiple training and test datasets.\n",
    "\n",
    "- **STEP 8: Decide on Model Deployment:** The final step in the regression model is to develop a deployment strategy in the form of actionable items and business rules that can be used by the organization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Linear Regression\n",
    "\n",
    "When performing linear regression, it is essential to evaluate the performance of the model to assess its accuracy and effectiveness. Several evaluation metrics can be used to measure the performance of a linear regression model. Here are some commonly used evaluation metrics:\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error measures the average squared difference between the predicted values and the actual values of the dependent variable. It is calculated by taking the average of the squared residuals.\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum \\left(y_i - \\hat{y}_i\\right)^2$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points.\n",
    "- $y_i$ is the actual value of the dependent variable for the $i-th$ data point.\n",
    "- $\\hat{y}_i$ is the predicted value of the dependent variable for the $i-th$ data point.\n",
    "\n",
    "A lower MSE value indicates better model performance, with zero being the best possible value.\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "The Root Mean Squared Error is the square root of the MSE and provides a more interpretable measure of the average prediction error.\n",
    "\n",
    "$\\text{RMSE} = \\sqrt{\\text{MSE}}$ \n",
    "\n",
    "Like the MSE, a lower RMSE value indicates better model performance.\n",
    "\n",
    "### 3. Mean Absolute Error (MAE)\n",
    "\n",
    "The Mean Absolute Error measures the average absolute difference between the predicted values and the actual values of the dependent variable. It is less sensitive to outliers compared to MSE.\n",
    "\n",
    "$\\text{MAE} = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|$\n",
    "\n",
    "A lower MAE value indicates better model performance.\n",
    "\n",
    "### 4. R-squared ($R^2$) Coefficient of Determination\n",
    "\n",
    "The R-squared value represents the proportion of the variance in the dependent variable that is explained by the independent variables. It ranges from $0$ to $1$, where $1$ indicates that the model perfectly predicts the dependent variable.\n",
    "\n",
    "$R^2 = 1 - \\frac{\\text{SSR}}{\\text{SSt}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- SSR is the sum of squared residuals (predicted values minus the mean of the dependent variable).\n",
    "- SST is the total sum of squares (actual values minus the mean of the dependent variable).\n",
    "\n",
    "A higher $R^2$ value indicates a better fit of the model to the data.\n",
    "\n",
    "### 5. Adjusted R-squared\n",
    "\n",
    "The Adjusted R-squared accounts for the number of independent variables in the model. It penalizes the inclusion of irrelevant variables and rewards the inclusion of relevant variables.\n",
    "\n",
    "$\\text{Adjusted}~ R^2 = 1-\\left[\\frac{(1 - R²) * (n - 1)}{(n - p - 1)}\\right]$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $n$ is the number of data points.\n",
    "- $p$ is the number of independent variables.\n",
    "\n",
    "A higher Adjusted $R^2$ value indicates a better fit of the model while considering the complexity of the model.\n",
    "\n",
    "These evaluation metrics help assess the performance of a linear regression model by quantifying the accuracy of the predictions and the extent to which the independent variables explain the dependent variable. It is important to consider multiple metrics to gain a comprehensive understanding of the model's performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

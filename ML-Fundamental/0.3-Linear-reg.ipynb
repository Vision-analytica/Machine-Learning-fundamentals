{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression in Machine Learning\n",
    "\n",
    "Linear regression is a popular and widely used algorithm in machine learning for predicting continuous numeric values. It models the relationship between independent variables (input features) and a dependent variable (target variable) by fitting a linear equation to the observed data. In this section, we will provide a brief overview of linear regression, including the mathematical explanation and figures to aid understanding.\n",
    "\n",
    "<img src=\"ML-image/Linear-reg1.png\" width=\"350\" height=\"300\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation:\n",
    "The linear regression algorithm aims to find the best-fit line that represents the relationship between the input features ($x$) and the target variable ($y$). The equation for a simple linear regression can be expressed as: \n",
    "\n",
    "$y=mx+b$\n",
    "\n",
    "where\n",
    "\n",
    "- $y$ represents the target variable or the dependent variable we want to predict.\n",
    "- $x$ represents the input feature or the independent variable.\n",
    "- $m$ represents the slope of the line, which represents the rate of change of $y$ with respect to $x$.\n",
    "- $b$ represents the $y$-intercept, which is the value of $y$ when $x$ is equal to $0$.\n",
    "\n",
    "<img src=\"ML-image/Linear-reg0.png\" width=\"350\" height=\"300\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship of regression lines\n",
    "\n",
    "- A linear line showing the relationship between the dependent and independent variables is called a regression line. \n",
    "- A regression line can show two types of relationship:\n",
    "\n",
    "1. **Positive Linear Relationship:** If the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship.\n",
    "\n",
    "<img src=\"ML-image/pos-lin-reg.png\" width=\"300\" height=\"250\" />\n",
    "\n",
    "1. **Negative Linear Relationship:** If the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.\n",
    "\n",
    "<img src=\"ML-image/neg-lin-reg.png\" width=\"300\" height=\"250\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Linear Regression\n",
    "\n",
    "Linear regression can be further divided into two types of the algorithm:\n",
    "\n",
    "1. **Simple Linear Regression:** If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n",
    "2. **Multiple Linear regression:** If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple linear regression \n",
    "\n",
    "Simple linear regression (SLR) is a statistical model that focuses on the relationship between a single independent variable (or feature) and an outcome variable. In SLR, the functional relationship between the outcome variable and the regression coefficient is linear. This means that the mathematical function describing the relationship is a straight line and the impact of the independent variable on the outcome is represented by a linear regression coefficient.\n",
    "\n",
    "**Example:** predicting housing prices based on features such as the number of bedrooms, square footage, and location.\n",
    "\n",
    "#### Mathematical Explanation: \n",
    "Mathematically, we can represent a linear regression as:\n",
    "\n",
    "$y=\\beta_0 + \\beta_1 x +\\epsilon$\n",
    "\n",
    "where\n",
    "\n",
    "- $y$ = Dependent Variable (Target Variable)\n",
    "- $x$ = Independent Variable (predictor Variable)\n",
    "- $\\beta_0$ = intercept of the line (Gives an additional degree of freedom)\n",
    "- $\\beta_1$ = Linear regression coefficient (scale factor to each input value).\n",
    "- $\\epsilon$ = random error\n",
    "\n",
    "The goal of linear regression is to estimate the values of the regression coefficients\n",
    "\n",
    "<img src=\"ML-image/Multi-lin-reg.png\" width=\"500\" height=\"350\" />\n",
    "\n",
    "This algorithm explains the linear relationship between the dependent(output) variable $y$ and the independent(predictor) variable $x$ using a straight line  $y=\\beta_0+\\beta_1 x$.\n",
    "\n",
    "#### Goal\n",
    "\n",
    "- The goal of the linear regression algorithm is to get the best values for $\\beta_0$ and $\\beta_1$ to find the best fit line. \n",
    "- The best fit line is a line that has the least error which means the error between predicted values and actual values should be minimum.\n",
    "- For a datset with $n$ observation $(x_i, y_i)$, where $i=1,2,3...., n$ the above function can be written as follows\n",
    "\n",
    "    $y_i=\\beta_0 + \\beta_1 x_i +\\epsilon_i$\n",
    "\n",
    "    where $y_i$ is the value of the observation of the dependent variable (outcome variable) in the smaple, $x_i$ is the value of $ith$ observation of the independent variable or feature in the sample, $\\epsilon_i$ is the random error (also known as residuals) in predicting the value of $y_i$, $\\beta_0$ and $\\beta_i$ are the regression parameters (or regression coefficients or feature weights).\n",
    "\n",
    "#### Calculating the regression parameters\n",
    "\n",
    "In simple linear regression, there is only one independent variable ($x$) and one dependent variable ($y$). The parameters (coefficients) in simple linear regression can be calculated using the method of **ordinary least squares (OLS)**. The equations and formulas involved in calculating the parameters are as follows:\n",
    "\n",
    "1. **Model Representation:**\n",
    "\n",
    "    The simple linear regression model can be represented as:\n",
    "    $y = \\beta_0 + \\beta_1 x + \\epsilon$\n",
    "\n",
    "    So,\n",
    "\n",
    "    $\\epsilon = y -\\beta_0 - \\beta_1 x$.\n",
    "\n",
    "2. **Cost Function or mean squared error (MSE):**\n",
    "\n",
    "    The MSE, measures the average squared difference between the predicted values ($\\hat{y}$) and the actual values of the dependent variable ($y$). It is given by:\n",
    "\n",
    "    MSE = $\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $n$ is the number of data points.\n",
    "    - $y_i$ is the actual value of the dependent variable for the i-th data point.\n",
    "    - $\\hat{y}_i$ is the predicted value of the dependent variable for the $i-th$ data point.\n",
    "\n",
    "3. **Minimization of the Cost Function:**\n",
    "\n",
    "    The parameters $\\beta_0$ and $\\beta_1$ are estimated by minimizing the cost function. The formulas for calculating the parameter estimates are derived from the derivative of the cost function with respect to each parameter.\n",
    "\n",
    "    The parameter estimates are given by:\n",
    "\n",
    "    - $\\hat{\\beta_1} = \\frac{\\text{Cov}(x,y)}{Var(x)}$ \n",
    "    - $\\hat{\\beta_0} = \\text{y} - \\hat{\\beta_1}\\times \\text{mean}(x)$ \n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{\\beta_0}$ is the estimated $y$-intercept.\n",
    "    - $\\hat{\\beta_1}$ is the estimated slope.\n",
    "    - Cov$(x, y)$ is the covariance between $x$ and $y$.\n",
    "    - $\\text{Var}(x)$ is the variance of $x$.\n",
    "    - $\\text{mean}(x)$ is the mean of $x$.\n",
    "    - $\\text{mean}(y)$ is the mean of $y$.\n",
    "\n",
    "    The estimated parameters $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ provide the values of the intercept and slope that best fit the data according to the simple linear regression model.\n",
    "\n",
    "4. **Prediction:**\n",
    "\n",
    "    Once the parameter estimates are obtained, predictions can be made using the equation:\n",
    "\n",
    "    $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{y}$ is the predicted value of the dependent variable.\n",
    "    - $\\hat{\\beta_0}$ is the estimated y-intercept.\n",
    "    - $\\hat{\\beta_1}$ is the estimated slope.\n",
    "    - $x$ is the value of the independent variable for which the prediction is being made.\n",
    "\n",
    "    These equations and formulas allow for the calculation of the parameters in simple linear regression using the method of **ordinary least squares (OLS)**. By minimizing the sum of squared differences between predicted and actual values, the parameters are determined to best fit the data and enable prediction of the dependent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiple linear regression algorithms\n",
    "\n",
    "Multiple Linear Regression is an extension of Simple Linear Regression that allows for the analysis of the relationship between a dependent variable ($y$) and multiple independent variables ($x_1$, $x_2$, ..., $x_p$). It assumes a linear relationship between the variables, with the objective of finding the best-fit hyperplane in a multi-dimensional space.\n",
    "\n",
    "#### Mathematical Explanation:\n",
    "\n",
    "The multiple linear regression model can be expressed mathematically as:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$ is the dependent variable.\n",
    "- $x_1$, $x_2$, ...., $x_p$ are the independent variables.\n",
    "- $\\beta_0$ is the $y$-intercept, representing the value of $y$ when all the independent variables are zero.\n",
    "- $\\beta_1$, $\\beta_2$, ...,  $\\beta_p$ are the coefficients associated with each independent variable, indicating the impact of each variable on the dependent variable while holding other variables constant.\n",
    "- $\\epsilon$ is the error term, representing the random variability or noise in the relationship between the variables.\n",
    "\n",
    "#### Goal \n",
    "The goal of multiple linear regression is to estimate the values of $\\beta_0$, $\\beta_1$, $\\beta_2$, ...,  $\\beta_p$ that provide the best fit to the observed data, minimizing the sum of squared differences between the predicted values ($\\hat{y}$) and the actual values of $y$.\n",
    "\n",
    "#### Calculating the regression parameters \n",
    "\n",
    "In linear regression, the parameters (coefficients) are estimated using a method called ordinary least squares (OLS). The goal of OLS is to find the values of the parameters that minimize the sum of squared differences between the predicted values and the actual values of the dependent variable. The equations and formulas involved in calculating the linear regression parameters are as follows:\n",
    "\n",
    "1. **Cost Function or mean squared error (MSE):**\n",
    "\n",
    "    The cost function, measures the average squared difference between the predicted values ($\\hat{y}$) and the actual values of the dependent variable ($y$). It is given by:\n",
    "\n",
    "    $\\text{MSE} = \\frac{1}{n} \\sum(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $n$ is the number of data points.\n",
    "    - $y_i$ is the actual value of the dependent variable for the $i-th$ data point.\n",
    "    - $\\hat{y}_i$ is the predicted value of the dependent variable for the $i-th$ data point.\n",
    "\n",
    "2. **Minimization of the Cost Function:**\n",
    "    The parameters $\\beta_0$, $\\beta_1$, $\\beta_2$, ...,  $\\beta_p$ are estimated by minimizing the cost function. The formulas for calculating the parameter estimates are derived from the derivative of the cost function with respect to each parameter.\n",
    "\n",
    "    The parameter estimates are given by:\n",
    "\n",
    "    $\\hat{\\beta} = \\left(X^T X\\right)^{-1}\\, X^T y$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{\\beta}$ is the vector of parameter estimates.\n",
    "    - $X$ is the design matrix consisting of the independent variables.\n",
    "    - $X^T$ is the transpose of the design matrix.\n",
    "    - $\\left(X^T X \\right)^{-1}$ is the inverse of the matrix product $X^TX$.\n",
    "    - $y$ is the vector of the dependent variable values.\n",
    "    \n",
    "    The estimated parameter vector $\\hat{\\beta}$ provides the values of the coefficients that best fit the data according to the linear regression model.\n",
    "\n",
    "3. **Prediction:**\n",
    "    Once the parameter estimates are obtained, predictions can be made using the equation:\n",
    "\n",
    "    $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + ... + \\hat{\\beta_p} x_p$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $\\hat{y}$ is the predicted value of the dependent variable.\n",
    "    - $\\hat{\\beta_0}$, $\\hat{\\beta_1}$, $\\hat{\\beta_2}$, ...,  $\\hat{\\beta_p}$ are the estimated coefficients.\n",
    "    - $x_1$, $x_2$, ..., $x_p$ are the values of the independent variables for which the prediction is being made.\n",
    "    \n",
    "    These equations and formulas form the basis of ordinary least squares (OLS) estimation for linear regression. By minimizing the sum of squared differences between predicted and actual values, the parameters are determined to best fit the data and allow for prediction of the dependent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in building a regression model\n",
    "\n",
    "- **STEP 1: Collect/Extract Data:** The first step in building a regression model is to collect or extract data on the dependent (outcome) variable and independent (feature) variables from different data sources.\n",
    "\n",
    "- **STEP 2: Pre-Process the Data:** Before the model is built, it is essential to ensure the quality of the data for issues such as reliability, com- pleteness, usefulness, accuracy, missing data, and outliers.\n",
    "\n",
    "    - Data imputation techniques may be used to deal with missing data. Use of descriptive statistics and visualization (such as box plot and scatter plot) may be used to identify the existence of outliers and variability in the dataset.\n",
    "    - Many new variables (such as the ratio of variables or product of variables) can be derived (aka feature engineering) and also used in model building.\n",
    "    - Categorical data must be pre-processed using dummy variables as a part of feature engineering, prior to utilizing it in a regression model.\n",
    "\n",
    "- **STEP 3: Dividing Data into Training and Validation Datasets:** In this stage the data is divided into two subsets (sometimes more than two subsets):\n",
    "\n",
    "    - training dataset and\n",
    "    - validation or test dataset.\n",
    "    \n",
    "    The proportion of training dataset is usually between 70% and 80% of the data and the remaining data is treated as the validation data. The subsets may be created using random/ stratified sampling procedure. This is an important step to measure the performance of the model using dataset not used in model building. It is also essential to check for any overfitting of the model. In many cases, multiple training and multiple test data are used (called cross-validation).\n",
    "\n",
    "- **STEP 4: Perform Descriptive Analytics or Data Exploration:** It is always a good practice to perform descriptive analytics before moving to building a predictive analytics model. Descriptive statistics will help us to understand the variability in the model and visualization of the data through, say, a box plot which will show if there are any outliers in the data. Another visualization technique, the scatter plot, may also reveal if there is any obvious relationship between the two variables under consideration. Scatter plot is useful to describe the functional relationship between the dependent or outcome variable and features.\n",
    "\n",
    "- **STEP 5: Build the Model:** The model is built using the training dataset to estimate the regression parameters. The method of Ordinary Least Squares (OLS) is used to estimate the regression parameters.\n",
    "\n",
    "- **STEP 6: Perform Model Diagnostics:** Regression is often misused since many times the modeler fails to perform necessary diagnostics tests before applying the model. Before it can be applied, it is necessary that the model created is validated for all model assumptions including the definition of the function form. If the model assumptions are violated, then the modeler must use remedial measure.\n",
    "\n",
    "- **STEP 7: Validate the Model and Measure Model Accuracy:** A major concern in analytics is over-fitting, that is, the model may perform very well on the training dataset, but may perform badly in validation dataset. It is important to ensure that the model perfor- mance is consistent on the validation dataset as is in the training dataset. In fact, the model may be cross- validated using multiple training and test datasets.\n",
    "\n",
    "- **STEP 8: Decide on Model Deployment:** The final step in the regression model is to develop a deployment strategy in the form of actionable items and business rules that can be used by the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms\n",
    "\n",
    "Regression algorithms in machine learning are used for predicting continuous numeric values or estimating a target variable based on input features. They model the relationship between the independent variables and the dependent variable by fitting a mathematical equation to the observed data. Here are some common regression algorithms along with their explanations and examples:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Linear Regression:** Linear regression is a simple and widely used algorithm. It assumes a linear relationship between the independent variables and the target variable. The algorithm estimates the coefficients of the linear equation that best fits the data. The equation can be of the form y = mx + b, where y is the target variable, x is the input feature, m is the slope, and b is the intercept. Example applications include predicting housing prices based on features like square footage and number of bedrooms, or estimating sales based on advertising expenditure.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Polynomial Regression:** Polynomial regression is an extension of linear regression where the relationship between the variables is modeled using a polynomial equation. This allows for more flexibility in capturing nonlinear relationships between the input features and the target variable. It involves adding polynomial terms, such as x^2 or x^3, to the linear equation. Polynomial regression is useful when the data exhibits curvilinear patterns. For example, it can be used to predict a person's weight based on their height, accounting for the potential nonlinearity in the relationship.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Ridge Regression:** Ridge regression is a regularization technique that addresses the issue of overfitting in linear regression. It adds a penalty term to the linear regression equation to control the complexity of the model. This penalty term helps prevent the coefficients from becoming too large, reducing the model's sensitivity to the training data. Ridge regression is particularly useful when dealing with high-dimensional data or when multicollinearity (high correlation) exists among the input features.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Lasso Regression:** Lasso regression, similar to ridge regression, is a regularization technique used to combat overfitting. It adds a penalty term to the linear regression equation, but in this case, it uses the L1 norm of the coefficients as the penalty. Lasso regression has a feature selection property that can drive some coefficients to zero, effectively performing automatic feature selection. This makes it useful when dealing with datasets with many features or when looking to identify the most influential variables.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **ElasticNet Regression:** ElasticNet regression combines both ridge and lasso regularization techniques. It adds a penalty term that is a linear combination of the L1 (lasso) and L2 (ridge) norms of the coefficients. This hybrid approach allows for feature selection while also providing stability and reducing the impact of multicollinearity. ElasticNet regression is useful when there are many correlated features and the goal is to both select relevant features and mitigate multicollinearity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Machine Learning\n",
    "\n",
    "Linear regression is a statistical method used to examine the relationship between a dependent variable (also called the response or outcome variable) and one or more independent variables (also called predictor or explanatory variables).\n",
    "\n",
    "A linear regression model assumes that there is a linear relationship between the dependent variable and the independent variables. That is, it assumes that a change in the independent variables will result in a proportional change in the dependent variable.\n",
    "\n",
    "**Example:** predicting housing prices based on features such as the number of bedrooms, square footage, and location.\n",
    "\n",
    "<img src=\"Linear-reg1.png\" width=\"350\" height=\"300\" />\n",
    "\n",
    "Mathematically, we can represent a linear regression as:\n",
    "\n",
    "$y=\\beta_0+ \\beta_1 x + \\epsilon$\n",
    "\n",
    "where\n",
    "\n",
    "- $y$= Dependent Variable (Target Variable)\n",
    "- $x$= Independent Variable (predictor Variable)\n",
    "- $\\beta_0$= intercept of the line (Gives an additional degree of freedom)\n",
    "- $\\beta_1$ = Linear regression coefficient (scale factor to each input value).\n",
    "- $\\epsilon$ = random error\n",
    "\n",
    "The goal of linear regression is to estimate the values of the regression coefficients "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
